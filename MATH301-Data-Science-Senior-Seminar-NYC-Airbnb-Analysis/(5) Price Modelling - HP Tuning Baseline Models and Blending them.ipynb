{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import Libraries, Read in Data and Merge into one big dataset with year indicator variable"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom wordcloud import STOPWORDS\nimport string\nimport datetime","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read in wrangled NYC airbnb data from 2016-2020\n\ndf16 = pd.read_csv(\"../input/math301-final-project-data/airbnb16.csv\", index_col=0)\ndf17 = pd.read_csv(\"../input/math301-final-project-data/airbnb17.csv\", index_col=0)\ndf18 = pd.read_csv(\"../input/math301-final-project-data/airbnb18.csv\", index_col=0)\ndf19 = pd.read_csv(\"../input/math301-final-project-data/airbnb19.csv\", index_col=0)\ndf20 = pd.read_csv(\"../input/math301-final-project-data/airbnb20.csv\", index_col=0)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add year variable to all the data sources\nfor dataf,year in zip([df16,df17,df18,df19,df20], ['16','17','18','19','20']):\n    dataf['year'] = '20' + year","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge data sources into one big dataset\ndf = pd.concat([df16,df17,df18,df19,df20])","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding Features + Feature Engineering + Split into train, test data"},{"metadata":{},"cell_type":"markdown","source":"#### Feature Creation from \"name\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['name'].fillna('missing',inplace=True) # fill ONE empty value in \"name\" column with word \"missing\"\n\n########## Basic Meta Features for Text data\n\n# word_count\ndf['name_wc'] = df['name'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf['name_unique_wc'] = df['name'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ndf['name_stop_wc'] = df['name'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# mean_word_length\ndf['name_mean_wl'] = df['name'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf['name_cc'] = df['name'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndf['name_pc'] = df['name'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# Drop \"name\" column\ndel df['name']","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop socioeconomic, demographic variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop socioeconomic, demographic variables\n\nfor feat in df.columns.tolist():\n    if 'avg' in feat:\n        del df[feat]\n    elif 'total' in feat:\n        del df[feat]","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create features for last_review"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create features for last_review (last review month, last review day)\n\ndf1 = df.copy()[df.last_review != 'No Review']\ndf2 = df.copy()[df.last_review == 'No Review']\n\ndf1['last_review_month'] = df1['last_review'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").month)\ndf1['last_review_day'] = df1['last_review'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").day)\n\ndf2['last_review_month'] = 0\ndf2['last_review_day'] = 0\n\ndf = pd.concat([df1,df2], axis=0, sort=True)\n\ndel df['last_review']","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Encode categorical variables into numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx =  df[df.year != '2020'].index\ntest_idx = df[df.year == '2020'].index","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dummy variables for categorical features\n\ndf = pd.get_dummies(df, columns= ['neighbourhood_group', 'room_type','year'], drop_first=True)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encode Neighborhoods (because too many variables will be created if we take the dummy variable approach)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder= LabelEncoder()\nnb_encoded = label_encoder.fit_transform(df[['neighbourhood']].values.ravel())","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace neighborhood column with integer label encoded data\n\ndf['neighbourhood'] = pd.Series(nb_encoded)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split into train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.copy().iloc[train_idx] # 2016-2019\ntest = df.copy().iloc[test_idx] # 2020","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide into X and y vectors/arrays\n\nX_train = train.drop(labels=['price'],axis=1).values\ny_train = train[['price']].values\n\nX_test = test.drop(labels=['price'],axis=1).values\ny_test = test[['price']].values","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save encoded data locally as csv files\n\ndf.to_csv(\"encoded_data.csv\")\ntrain.to_csv(\"train.csv\")\ntest.to_csv(\"test.csv\")","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline Models with HyperParameter Tuning and some tinkering (e.g. standardization, Polynomial features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ridge Regression - HP Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression - Hyperparameter Tuning with RandomizedCV\n\nridge = Ridge()\n\nridge_alpha_params = list(np.linspace(0.01, 1.5))\n\nridge_param_dist = dict(alpha=ridge_alpha_params)\n\nridge_rand = RandomizedSearchCV(ridge, ridge_param_dist, cv=10, scoring='neg_median_absolute_error', n_iter=50, random_state=42)\n\n# fit\nridge_rand.fit(X_train, y_train)\n\n# Scores\n# ridge_rand.grid_scores_\n\n# Examine the best model\nprint(-(ridge_rand.best_score_))\nprint(ridge_rand.best_params_)\nprint(ridge_rand.best_estimator_)","execution_count":16,"outputs":[{"output_type":"stream","text":"31.51131323108275\n{'alpha': 1.4695918367346938}\nRidge(alpha=1.4695918367346938, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"When we tried linspace(0.01, 1.5), we got about 1.469 as out best alpha parameter. Let's try RandomizedCV again in a different range closer to 1.469 (maybe 1.0 - 2.0 ?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression - Hyperparameter Tuning with RandomizedCV with different range\n\nridge = Ridge()\n\nridge_alpha_params = list(np.linspace(1.0, 3.0))\n\nridge_param_dist = dict(alpha=ridge_alpha_params)\n\nridge_rand = RandomizedSearchCV(ridge, ridge_param_dist, cv=10, scoring='neg_median_absolute_error', n_iter=30, random_state=50)\n\n# fit\nridge_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(ridge_rand.best_score_))\nprint(ridge_rand.best_params_)\nprint(ridge_rand.best_estimator_)","execution_count":17,"outputs":[{"output_type":"stream","text":"31.487369739797394\n{'alpha': 2.7959183673469385}\nRidge(alpha=2.7959183673469385, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression - Hyperparameter Tuning with RandomizedCV with different range (This time linspace(2.7, 5.0))\n\nridge = Ridge()\n\nridge_alpha_params = list(np.linspace(2.7, 5.0))\n\nridge_param_dist = dict(alpha=ridge_alpha_params)\n\nridge_rand = RandomizedSearchCV(ridge, ridge_param_dist, cv=10, scoring='neg_median_absolute_error', n_iter=30, random_state=50)\n\n# fit\nridge_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(ridge_rand.best_score_))\nprint(ridge_rand.best_params_)\nprint(ridge_rand.best_estimator_)","execution_count":18,"outputs":[{"output_type":"stream","text":"31.465771592189775\n{'alpha': 4.3428571428571425}\nRidge(alpha=4.3428571428571425, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression - Hyperparameter Tuning with RandomizedCV with different range (This time linspace(4, 10))\n\nridge = Ridge()\n\nridge_alpha_params = list(np.linspace(4, 10))\n\nridge_param_dist = dict(alpha=ridge_alpha_params)\n\nridge_rand = RandomizedSearchCV(ridge, ridge_param_dist, cv=10, scoring='neg_median_absolute_error', n_iter=30, random_state=50)\n\n# fit\nridge_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(ridge_rand.best_score_))\nprint(ridge_rand.best_params_)\nprint(ridge_rand.best_estimator_)","execution_count":19,"outputs":[{"output_type":"stream","text":"31.455988846702894\n{'alpha': 8.285714285714285}\nRidge(alpha=8.285714285714285, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression - Hyperparameter Tuning with RandomizedCV with different range (This time linspace(8, 100))\n\nridge = Ridge()\n\nridge_alpha_params = list(np.linspace(8, 100))\n\nridge_param_dist = dict(alpha=ridge_alpha_params)\n\nridge_rand = RandomizedSearchCV(ridge, ridge_param_dist, cv=10, scoring='neg_median_absolute_error', n_iter=30, random_state=50)\n\n# fit\nridge_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(ridge_rand.best_score_))\nprint(ridge_rand.best_params_)\nprint(ridge_rand.best_estimator_)","execution_count":20,"outputs":[{"output_type":"stream","text":"31.45363542922478\n{'alpha': 23.02040816326531}\nRidge(alpha=23.02040816326531, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression - Hyperparameter Tuning with RandomizedCV with different range (This time linspace(8, 100))\n\nridge = Ridge()\n\nridge_alpha_params = list(np.linspace(22, 24))\n\nridge_param_dist = dict(alpha=ridge_alpha_params)\n\nridge_gs = GridSearchCV(ridge, ridge_param_dist, cv=10, scoring='neg_median_absolute_error')\n\n# fit\nridge_gs.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(ridge_gs.best_score_))\nprint(ridge_gs.best_params_)\nprint(ridge_gs.best_estimator_)","execution_count":21,"outputs":[{"output_type":"stream","text":"31.444239435543388\n{'alpha': 22.0}\nRidge(alpha=22.0, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=None, solver='auto', tol=0.001)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression with the best parameter found from RandomizedCV + GridSearchCV\n\nridge = Ridge(alpha=22.0)\n\nprint(\"Ridge Cross Validation MSE: {}\".format(round(-cross_val_score(ridge, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean(),2)))\nprint(\"Ridge Cross Validation Median Absolute Error: {}\".format(round(-cross_val_score(ridge, X_train, y_train, cv=5, scoring='neg_median_absolute_error').mean(),2)))\nprint(\"Ridge Cross Validation Mean Absolute Error: {}\".format(round(-cross_val_score(ridge, X_train, y_train, cv=5, scoring='neg_mean_absolute_error').mean(), 2)))\nprint(\"Ridge Cross Validation R-Squared: {}\".format(round(-cross_val_score(ridge, X_train, y_train, cv=5, scoring='r2').mean(), 2)))\n\nridge.fit(X_train,y_train)\n\nprint(\"Ridge Predicton MSE: {}\".format(round(mean_squared_error(y_test,ridge.predict(X_test)), 2 )))\nprint(\"Ridge Predicton Median Absolute Error: {}\".format(round(median_absolute_error(y_test, ridge.predict(X_test)),2 )))\nprint(\"Ridge Predicton Mean Absolute Error: {}\".format(round(mean_absolute_error(y_test, ridge.predict(X_test)), 2)))\nprint(\"Ridge Predicton R-Squared: {}\".format(round(r2_score(y_test, ridge.predict(X_test)), 2 )))","execution_count":22,"outputs":[{"output_type":"stream","text":"Ridge Cross Validation MSE: 24112.94\nRidge Cross Validation Median Absolute Error: 31.28\nRidge Cross Validation Mean Absolute Error: 50.5\nRidge Cross Validation R-Squared: -0.15\nRidge Predicton MSE: 22965.22\nRidge Predicton Median Absolute Error: 30.65\nRidge Predicton Mean Absolute Error: 49.53\nRidge Predicton R-Squared: 0.15\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Best median absolute error for Ridge Regression after HP tuning was 30.65 which is a 0.05 decrease from the baseline score without HP tuning"},{"metadata":{},"cell_type":"markdown","source":"#### Lasso Regression with HP Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regression - Hyperparameter Tuning with RandomizedCV\n\nlasso = Lasso()\n\nlasso_alpha_params = list(np.linspace(0.01, 0.2))\n\nlasso_param_dist = dict(alpha=lasso_alpha_params)\n\nlasso_rand = RandomizedSearchCV(lasso, lasso_param_dist, cv=10, scoring='neg_median_absolute_error', n_iter=30, random_state=42)\n\n# fit\nlasso_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(lasso_rand.best_score_))\nprint(lasso_rand.best_params_)\nprint(lasso_rand.best_estimator_)","execution_count":23,"outputs":[{"output_type":"stream","text":"31.410606656709568\n{'alpha': 0.08367346938775509}\nLasso(alpha=0.08367346938775509, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regression - Hyperparameter Tuning with RandomizedCV on a different range (0.07 - 1.2)\n\nlasso = Lasso()\n\nlasso_alpha_params = list(np.linspace(0.07, 1.2))\n\nlasso_param_dist = dict(alpha=lasso_alpha_params)\n\nlasso_rand = RandomizedSearchCV(lasso, lasso_param_dist, cv=5, scoring='neg_median_absolute_error', n_iter=15, random_state=42)\n\n# fit\nlasso_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(lasso_rand.best_score_))\nprint(lasso_rand.best_params_)\nprint(lasso_rand.best_estimator_)","execution_count":24,"outputs":[{"output_type":"stream","text":"31.28436968364913\n{'alpha': 0.25448979591836735}\nLasso(alpha=0.25448979591836735, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regression - Hyperparameter Tuning with RandomizedCV on a different range (0.2 - 0.5)\n\nlasso = Lasso()\n\nlasso_alpha_params = list(np.linspace(0.2, 0.5))\n\nlasso_param_dist = dict(alpha=lasso_alpha_params)\n\nlasso_rand = RandomizedSearchCV(lasso, lasso_param_dist, cv=5, scoring='neg_median_absolute_error', n_iter=15, random_state=42)\n\n# fit\nlasso_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(lasso_rand.best_score_))\nprint(lasso_rand.best_params_)\nprint(lasso_rand.best_estimator_)","execution_count":25,"outputs":[{"output_type":"stream","text":"31.272616610606384\n{'alpha': 0.2489795918367347}\nLasso(alpha=0.2489795918367347, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regression - Hyperparameter Tuning with RandomizedCV on a different range (0.1 - 0.3)\n\nlasso = Lasso()\n\nlasso_alpha_params = list(np.linspace(0.1, 0.3))\n\nlasso_param_dist = dict(alpha=lasso_alpha_params)\n\nlasso_rand = RandomizedSearchCV(lasso, lasso_param_dist, cv=5, scoring='neg_median_absolute_error', n_iter=15, random_state=42)\n\n# fit\nlasso_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(lasso_rand.best_score_))\nprint(lasso_rand.best_params_)\nprint(lasso_rand.best_estimator_)","execution_count":26,"outputs":[{"output_type":"stream","text":"31.277783590178295\n{'alpha': 0.2510204081632653}\nLasso(alpha=0.2510204081632653, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regression - Hyperparameter Tuning with GridSearchCV\n\nlasso = Lasso()\n\nlasso_alpha_params = list(np.linspace(0.1, 0.25))\n\nlasso_param_dist = dict(alpha=lasso_alpha_params)\n\nlasso_gs = GridSearchCV(lasso, lasso_param_dist, cv=5, scoring='neg_median_absolute_error')\n\n# fit\nlasso_gs.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(lasso_gs.best_score_))\nprint(lasso_gs.best_params_)\nprint(lasso_gs.best_estimator_)","execution_count":27,"outputs":[{"output_type":"stream","text":"31.24204763772941\n{'alpha': 0.1}\nLasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"No gains from HP tuning. The best parameter alpha = 0.1 was the value we already used for baseline, so that being said, the alpha value we initially set for the baseline model already had the optimal parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Regression\n\nlasso = Lasso(alpha=0.1)\n\nprint(\"Lasso Cross Validation MSE: {}\".format(round(-cross_val_score(lasso, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean(),2)))\nprint(\"Lasso Cross Validation Median Absolute Error: {}\".format(round(-cross_val_score(lasso, X_train, y_train, cv=5, scoring='neg_median_absolute_error').mean(),2)))\nprint(\"Lasso Cross Validation Mean Absolute Error: {}\".format(round(-cross_val_score(lasso, X_train, y_train, cv=5, scoring='neg_mean_absolute_error').mean(), 2)))\nprint(\"Lasso Cross Validation R-Squared: {}\".format(round(-cross_val_score(lasso, X_train, y_train, cv=5, scoring='r2').mean(), 2)))\n\nlasso.fit(X_train,y_train)\n\nprint(\"Lasso Predicton MSE: {}\".format(round(mean_squared_error(y_test,lasso.predict(X_test)), 2 )))\nprint(\"Lasso Predicton Median Absolute Error: {}\".format(round(median_absolute_error(y_test, lasso.predict(X_test)),2 )))\nprint(\"Lasso Predicton Mean Absolute Error: {}\".format(round(mean_absolute_error(y_test, lasso.predict(X_test)), 2)))\nprint(\"Lasso Predicton R-Squared: {}\".format(round(r2_score(y_test, lasso.predict(X_test)), 2 )))","execution_count":28,"outputs":[{"output_type":"stream","text":"Lasso Cross Validation MSE: 24129.95\nLasso Cross Validation Median Absolute Error: 31.24\nLasso Cross Validation Mean Absolute Error: 50.48\nLasso Cross Validation R-Squared: -0.15\nLasso Predicton MSE: 22983.24\nLasso Predicton Median Absolute Error: 30.59\nLasso Predicton Mean Absolute Error: 49.49\nLasso Predicton R-Squared: 0.15\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### ElasticNet with HP Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ElasticNet Regression - Hyperparameter Tuning with RandomizedCV\n\nen = ElasticNet()\n\nen_alpha_params = list(np.linspace(0.9, 1.0))\nen_l1r_params = list(np.linspace(0, 1))\n\nen_param_dist = dict(alpha=en_alpha_params, l1_ratio = en_l1r_params)\n\nen_rand = RandomizedSearchCV(en, en_param_dist, cv=5, scoring='neg_median_absolute_error', n_iter=10, random_state=42)\n\n# fit\nen_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(en_rand.best_score_))\nprint(en_rand.best_params_)\nprint(en_rand.best_estimator_)","execution_count":29,"outputs":[{"output_type":"stream","text":"34.02828446432686\n{'l1_ratio': 0.9183673469387754, 'alpha': 0.9428571428571428}\nElasticNet(alpha=0.9428571428571428, copy_X=True, fit_intercept=True,\n           l1_ratio=0.9183673469387754, max_iter=1000, normalize=False,\n           positive=False, precompute=False, random_state=None,\n           selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ElasticNet Regression - Hyperparameter Tuning with RandomizedCV on different ranges\n\nen = ElasticNet()\n\nen_alpha_params = list(np.linspace(0.94, 1.4))\nen_l1r_params = list(np.linspace(0.8, 1))\n\nen_param_dist = dict(alpha=en_alpha_params, l1_ratio = en_l1r_params)\n\nen_rand = RandomizedSearchCV(en, en_param_dist, cv=5, scoring='neg_median_absolute_error', n_iter=15, random_state=42)\n\n# fit\nen_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(en_rand.best_score_))\nprint(en_rand.best_params_)\nprint(en_rand.best_estimator_)","execution_count":30,"outputs":[{"output_type":"stream","text":"32.01679948437963\n{'l1_ratio': 0.9836734693877551, 'alpha': 1.137142857142857}\nElasticNet(alpha=1.137142857142857, copy_X=True, fit_intercept=True,\n           l1_ratio=0.9836734693877551, max_iter=1000, normalize=False,\n           positive=False, precompute=False, random_state=None,\n           selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ElasticNet Regression - Hyperparameter Tuning with RandomizedCV on different ranges\n\nen = ElasticNet()\n\nen_alpha_params = list(np.linspace(1.0, 1.5))\nen_l1r_params = list(np.linspace(0.98, 1.0))\n\nen_param_dist = dict(alpha=en_alpha_params, l1_ratio = en_l1r_params)\n\nen_rand = RandomizedSearchCV(en, en_param_dist, cv=5, scoring='neg_median_absolute_error', n_iter=15, random_state=42)\n\n# fit\nen_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(en_rand.best_score_))\nprint(en_rand.best_params_)\nprint(en_rand.best_estimator_)","execution_count":31,"outputs":[{"output_type":"stream","text":"31.71654719575546\n{'l1_ratio': 0.9983673469387755, 'alpha': 1.2142857142857142}\nElasticNet(alpha=1.2142857142857142, copy_X=True, fit_intercept=True,\n           l1_ratio=0.9983673469387755, max_iter=1000, normalize=False,\n           positive=False, precompute=False, random_state=None,\n           selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ElasticNet Regression - Hyperparameter Tuning with RandomizedCV on different ranges\n\nen = ElasticNet()\n\nen_alpha_params = list(np.linspace(1.2, 1.8))\nen_l1r_params = list(np.linspace(0.98, 1.0))\n\nen_param_dist = dict(alpha=en_alpha_params, l1_ratio = en_l1r_params)\n\nen_rand = RandomizedSearchCV(en, en_param_dist, cv=5, scoring='neg_median_absolute_error', n_iter=15, random_state=42)\n\n# fit\nen_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(en_rand.best_score_))\nprint(en_rand.best_params_)\nprint(en_rand.best_estimator_)","execution_count":32,"outputs":[{"output_type":"stream","text":"31.710414198174295\n{'l1_ratio': 0.9983673469387755, 'alpha': 1.457142857142857}\nElasticNet(alpha=1.457142857142857, copy_X=True, fit_intercept=True,\n           l1_ratio=0.9983673469387755, max_iter=1000, normalize=False,\n           positive=False, precompute=False, random_state=None,\n           selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ElasticNet Regression - Hyperparameter Tuning with RandomizedCV on different ranges\n\nen = ElasticNet()\n\nen_alpha_params = list(np.linspace(1.4, 2.0))\nen_l1r_params = list(np.linspace(0.99, 1.0))\n\nen_param_dist = dict(alpha=en_alpha_params, l1_ratio = en_l1r_params)\n\nen_rand = RandomizedSearchCV(en, en_param_dist, cv=5, scoring='neg_median_absolute_error', n_iter=15, random_state=42)\n\n# fit\nen_rand.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(en_rand.best_score_))\nprint(en_rand.best_params_)\nprint(en_rand.best_estimator_)","execution_count":33,"outputs":[{"output_type":"stream","text":"31.715944184357006\n{'l1_ratio': 0.9977551020408163, 'alpha': 1.7918367346938775}\nElasticNet(alpha=1.7918367346938775, copy_X=True, fit_intercept=True,\n           l1_ratio=0.9977551020408163, max_iter=1000, normalize=False,\n           positive=False, precompute=False, random_state=None,\n           selection='cyclic', tol=0.0001, warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ElasticNet Regression - Hyperparameter Tuning with GridSearCV\n\nen = ElasticNet()\n\nen_alpha_params = list(np.linspace(1.4, 1.8))\nen_l1r_params = list(np.linspace(0.98, 1.0))\n\nen_param_dist = dict(alpha=en_alpha_params, l1_ratio = en_l1r_params)\n\nen_gs = GridSearchCV(en, en_param_dist, cv=5, scoring='neg_median_absolute_error')\n\n# fit\nen_gs.fit(X_train, y_train)\n\n# Examine the best model\nprint(-(en_gs.best_score_))\nprint(en_gs.best_params_)\nprint(en_gs.best_estimator_)","execution_count":34,"outputs":[{"output_type":"stream","text":"31.694797733020163\n{'alpha': 1.6612244897959183, 'l1_ratio': 1.0}\nElasticNet(alpha=1.6612244897959183, copy_X=True, fit_intercept=True,\n           l1_ratio=1.0, max_iter=1000, normalize=False, positive=False,\n           precompute=False, random_state=None, selection='cyclic', tol=0.0001,\n           warm_start=False)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ElasticNet\n\nen = ElasticNet(alpha=1.6612244897959183, copy_X=True, fit_intercept=True,\n           l1_ratio=1.0, max_iter=1000, normalize=False, positive=False,\n           precompute=False, random_state=None, selection='cyclic', tol=0.0001,\n           warm_start=False)\n\nprint(\"ElasticNet Cross Validation MSE: {}\".format(round(-cross_val_score(en, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean(),2)))\nprint(\"ElasticNet Cross Validation Median Absolute Error: {}\".format(round(-cross_val_score(en, X_train, y_train, cv=5, scoring='neg_median_absolute_error').mean(),2)))\nprint(\"ElasticNet Cross Validation Mean Absolute Error: {}\".format(round(-cross_val_score(en, X_train, y_train, cv=5, scoring='neg_mean_absolute_error').mean(), 2)))\nprint(\"ElasticNet Cross Validation R-Squared: {}\".format(round(-cross_val_score(en, X_train, y_train, cv=5, scoring='r2').mean(), 2)))\n\nen.fit(X_train,y_train)\n\nprint(\"ElasticNet Predicton MSE: {}\".format(round(mean_squared_error(y_test,en.predict(X_test)), 2 )))\nprint(\"ElasticNet Predicton Median Absolute Error: {}\".format(round(median_absolute_error(y_test, en.predict(X_test)),2 )))\nprint(\"ElasticNet Predicton Mean Absolute Error: {}\".format(round(mean_absolute_error(y_test, en.predict(X_test)), 2)))\nprint(\"ElasticNet Predicton R-Squared: {}\".format(round(r2_score(y_test, en.predict(X_test)), 2 )))","execution_count":35,"outputs":[{"output_type":"stream","text":"ElasticNet Cross Validation MSE: 24447.86\nElasticNet Cross Validation Median Absolute Error: 31.69\nElasticNet Cross Validation Mean Absolute Error: 51.44\nElasticNet Cross Validation R-Squared: -0.14\nElasticNet Predicton MSE: 23291.91\nElasticNet Predicton Median Absolute Error: 31.17\nElasticNet Predicton Mean Absolute Error: 50.5\nElasticNet Predicton R-Squared: 0.13\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Blending all three linear regularization methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_ridge_en_pred_blend = lasso.predict(X_test) * 0.8 + ridge.predict(X_test).reshape(26907,) * 0.15 + en.predict(X_test) * 0.05","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Blend Predicton MSE: {}\".format(round(mean_squared_error(y_test, lasso_ridge_en_pred_blend),2 )))\nprint(\"Blend Predicton Median Absolute Error: {}\".format(round(median_absolute_error(y_test, lasso_ridge_en_pred_blend),2 )))\nprint(\"Blend Predicton Mean Absolute Error: {}\".format(round(mean_absolute_error(y_test, lasso_ridge_en_pred_blend),2 )))\nprint(\"Blend Predicton R-Squared: {}\".format(round(r2_score(y_test, lasso_ridge_en_pred_blend),2 )))","execution_count":76,"outputs":[{"output_type":"stream","text":"Blend Predicton MSE: 22983.23\nBlend Predicton Median Absolute Error: 30.53\nBlend Predicton Mean Absolute Error: 49.44\nBlend Predicton R-Squared: 0.15\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}