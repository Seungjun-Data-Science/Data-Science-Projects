{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import Libraries and Read in Wrangled Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport spacy\nimport nltk\nimport string\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nimport re\nfrom bs4 import BeautifulSoup\nimport unicodedata\nfrom wordcloud import STOPWORDS\nimport gc\nimport re\nimport string\nimport operator\nfrom collections import defaultdict\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Covid Tweets\ncovid_twt_df = pd.read_csv(\"../input/2020-vassar-datafest-josh-data/covid_tweets.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Tweets from 2009\nbef_covid_twt_df = pd.read_csv(\"../input/2020-vassar-datafest-josh-data/sent_tweet.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# US Gov. response data to COVID\nus_resp_to_covid_df = pd.read_csv(\"../input/2020-vassar-datafest-josh-data/us_covid_resp.csv\")","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reddit Depression and non-depression posts for training\nreddit_dep_df = pd.read_csv(\"../input/reddit-depression-data/preprocessed_data.txt\")","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing for General Tweets & Covid Tweets (post March)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(document):\n    \n    \"\"\"\n    The clean_text function preprocesses the texts in a document/comment\n    \n    Parameters\n    ----------\n    document: the raw text\n    \n    Returns\n    ----------\n    tokens: a list of preprocessed tokens\n    \n    \"\"\"\n    \n    document = ' '.join([word.lower() for word in word_tokenize(document)]) # lowercase texts\n    tokens = word_tokenize(document) # tokenize the document\n    \n    for i in range(0,len(tokens)):\n        # remove whitespaces\n        tokens[i] = tokens[i].strip()\n        # remove html links\n        tokens[i] = re.sub(r'\\S*http\\S*', '', tokens[i]) # remove links with http\n        tokens[i] = re.sub(r'\\S*\\.org\\S*', '', tokens[i]) # remove links with .org\n        tokens[i] = re.sub(r'\\S*\\.com\\S*', '', tokens[i]) # remove links with .com\n        \n        # remove subreddit titles (e.g /r/food)\n        tokens[i] = re.sub(r'S*\\/r\\/\\S*', '' ,tokens[i]) \n        \n        # remove non-alphabet characters\n        tokens[i] = re.sub(\"[^a-zA-Z]+\", \"\", tokens[i])\n        \n        tokens[i] = tokens[i].strip() # remove whitespaces \n        \n        # remove all blanks from the list\n    while(\"\" in tokens): \n        tokens.remove(\"\") \n     \n    return tokens","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cleaning text for General Tweets pre-covid\n\n# call clean_text on df for each row in df\nfor i in range(0,len(bef_covid_twt_df)):\n    # use clean_text on the document/text stored in the content column\n    clean = clean_text(bef_covid_twt_df.loc[i,\"text\"])\n    # joining the tokens together by whitespaces\n    bef_covid_twt_df.loc[i,\"clean_content\"] = ' '.join([token for token in clean])\n    \nbef_covid_twt_df = bef_covid_twt_df.dropna() # remove null data due to some deleted comments\nbef_covid_twt_df = bef_covid_twt_df[bef_covid_twt_df[\"clean_content\"] != ''] # remove blank comments","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cleaning text for Covid Tweets post March\n\n# call clean_text on df for each row in df\nfor i in range(0,len(covid_twt_df)):\n    # use clean_text on the document/text stored in the content column\n    clean2 = clean_text(covid_twt_df.loc[i,\"text\"])\n    # joining the tokens together by whitespaces\n    covid_twt_df.loc[i,\"clean_content\"] = ' '.join([token for token in clean2])\n    \ncovid_twt_df = covid_twt_df.dropna() # remove null data due to some deleted comments\ncovid_twt_df = covid_twt_df[covid_twt_df[\"clean_content\"] != ''] # remove blank comments","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Meta Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Reddit Data\n\n# word_count\nreddit_dep_df['wc'] = reddit_dep_df['clean_content'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\nreddit_dep_df['unique_wc'] = reddit_dep_df['clean_content'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\nreddit_dep_df['stop_wc'] = reddit_dep_df['content'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# mean_word_length\nreddit_dep_df['mean_wl'] = reddit_dep_df['clean_content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# median_word_length\nreddit_dep_df['median_wl'] = reddit_dep_df['clean_content'].apply(lambda x: np.median([len(w) for w in str(x).split()]))\n\n# char_count\nreddit_dep_df['cc'] = reddit_dep_df['clean_content'].apply(lambda x: len(str(x)))\n\n# punctuation_count\nreddit_dep_df['pc'] = reddit_dep_df['content'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### General Tweets before Covid\n\n# word_count\nbef_covid_twt_df['wc'] = bef_covid_twt_df['clean_content'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\nbef_covid_twt_df['unique_wc'] = bef_covid_twt_df['clean_content'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\nbef_covid_twt_df['stop_wc'] = bef_covid_twt_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# mean_word_length\nbef_covid_twt_df['mean_wl'] = bef_covid_twt_df['clean_content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# median_word_length\nbef_covid_twt_df['median_wl'] = bef_covid_twt_df['clean_content'].apply(lambda x: np.median([len(w) for w in str(x).split()]))\n\n# char_count\nbef_covid_twt_df['cc'] = bef_covid_twt_df['clean_content'].apply(lambda x: len(str(x)))\n\n# punctuation_count\nbef_covid_twt_df['pc'] = bef_covid_twt_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### COVID Tweets\n\n# word_count\ncovid_twt_df['wc'] = covid_twt_df['clean_content'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ncovid_twt_df['unique_wc'] = covid_twt_df['clean_content'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ncovid_twt_df['stop_wc'] = covid_twt_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# mean_word_length\ncovid_twt_df['mean_wl'] = covid_twt_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# median_word_length\ncovid_twt_df['median_wl'] = covid_twt_df['clean_content'].apply(lambda x: np.median([len(w) for w in str(x).split()]))\n\n# char_count\ncovid_twt_df['cc'] = covid_twt_df['clean_content'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ncovid_twt_df['pc'] = covid_twt_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Depression Classifier Model using Reddit Data - n-grams / tfidf + Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"#### Train Reddit Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# n-gram features\n\ncountvec = CountVectorizer(ngram_range=(1,5),max_features=100,analyzer='word')\ncountvec_features = countvec.fit_transform(reddit_dep_df['clean_content'])","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tf-idf features \ntfidf = TfidfVectorizer(ngram_range=(1,5),max_features=100,analyzer='word')\ntfidf_features = tfidf.fit_transform(reddit_dep_df['clean_content'])","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n-gram features + tf-idf features + Meta Features I created\nfeats = pd.concat([reddit_dep_df.iloc[:,3:], pd.DataFrame(countvec_features.toarray()), pd.DataFrame(tfidf_features.toarray())], axis=1)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Labels\nlabels = reddit_dep_df.label","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Naive Bayes: CV Accuracy \", cross_val_score(nb, feats, labels, scoring='accuracy', cv=5).mean())\n\nprint(\"Naive Bayes: CV AUC Score \", cross_val_score(nb, feats, labels, scoring='roc_auc', cv=5).mean())\n\nprint(\"Naive Bayes: CV F1 Score \", cross_val_score(nb, feats, labels, scoring='f1', cv=5).mean())","execution_count":17,"outputs":[{"output_type":"stream","text":"Naive Bayes: CV Accuracy  0.7785929060963495\nNaive Bayes: CV AUC Score  0.8903250350059306\nNaive Bayes: CV F1 Score  0.7450338803981348\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nnb.fit(feats, labels)","execution_count":18,"outputs":[{"output_type":"stream","text":"CPU times: user 16 ms, sys: 1 ms, total: 17 ms\nWall time: 15 ms\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"GaussianNB(priors=None, var_smoothing=1e-09)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### General Tweets in 2009"},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Tweets post covid : n-gram features\n\ngtwt_countvec = CountVectorizer(ngram_range=(1,5),max_features=100,analyzer='word')\ngtwt_countvec_features = gtwt_countvec.fit_transform(bef_covid_twt_df['clean_content'])","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Tweets post covid : tf-idf features \ngtwt_tfidf = TfidfVectorizer(ngram_range=(1,5),max_features=100,analyzer='word')\ngtwt_tfidf_features = gtwt_tfidf.fit_transform(bef_covid_twt_df['clean_content'])","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Tweets post covid : n-gram features + tf-idf features + Meta Features I created\ngtwt_feats = pd.concat([bef_covid_twt_df.iloc[:,3:], pd.DataFrame(gtwt_countvec_features.toarray()), pd.DataFrame(gtwt_tfidf_features.toarray())], axis=1)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df['nb_predicted_depression'] = nb.predict(gtwt_feats)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df.nb_predicted_depression.value_counts() * 100 / len(bef_covid_twt_df)","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"0    90.844697\n1     9.155303\nName: nb_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### COVID tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Covid Tweets : n-gram features\n\ncovid_twt_countvec = CountVectorizer(ngram_range=(1,5),max_features=100,analyzer='word')\ncovid_twt_countvec_features = covid_twt_countvec.fit_transform(covid_twt_df['clean_content'])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Covid Tweets : tf-idf features \ncovid_twt_tfidf = TfidfVectorizer(ngram_range=(1,5),max_features=100,analyzer='word')\ncovid_twt_tfidf_features = covid_twt_tfidf.fit_transform(covid_twt_df['clean_content'])","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Covid Tweets : n-gram features + tf-idf features + Meta Features I created\ncovid_twt_feats = pd.concat([covid_twt_df.iloc[:,12:], pd.DataFrame(covid_twt_countvec_features.toarray()), pd.DataFrame(covid_twt_tfidf_features.toarray())], axis=1)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df['nb_predicted_depression'] = nb.predict(covid_twt_feats)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df.nb_predicted_depression.value_counts() * 100 / len(covid_twt_df)","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"0    93.463595\n1     6.536405\nName: nb_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Depression Classifier Model using Reddit Data - n-grams / tf-idf + Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"#### Reddit Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(max_iter=10000)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic Regression: CV Accuracy \", cross_val_score(logit, feats, labels, scoring='accuracy', cv=5).mean())\n\nprint(\"Logistic Regression: CV AUC Score \", cross_val_score(logit, feats, labels, scoring='roc_auc', cv=5).mean())\n\nprint(\"Logistic Regression: CV F1 Score \", cross_val_score(logit, feats, labels, scoring='f1', cv=5).mean())","execution_count":35,"outputs":[{"output_type":"stream","text":"Logistic Regression: CV Accuracy  0.8841777065437881\nLogistic Regression: CV AUC Score  0.9504855819507417\nLogistic Regression: CV F1 Score  0.8811411231154604\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nlogit.fit(feats, labels)","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Before Covid - General Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df['lr_predicted_depression'] = logit.predict(gtwt_feats)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df.lr_predicted_depression.value_counts() * 100 / len(bef_covid_twt_df)","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"0    73.416667\n1    26.583333\nName: lr_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Covid Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df['lr_predicted_depression'] = logit.predict(covid_twt_feats)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df.lr_predicted_depression.value_counts() * 100 / len(covid_twt_df)","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"0    83.924311\n1    16.075689\nName: lr_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Depression Classifier Model using Reddit Data - n-grams / tf-idf + Random Forest"},{"metadata":{},"cell_type":"markdown","source":"#### Reddit Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 1000)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"RF: CV Accuracy \", cross_val_score(rf, feats, labels, scoring='accuracy', cv=5).mean())\n\nprint(\"RF: CV AUC Score \", cross_val_score(rf, feats, labels, scoring='roc_auc', cv=5).mean())\n\nprint(\"RF: CV F1 Score \", cross_val_score(rf, feats, labels, scoring='f1', cv=5).mean())","execution_count":43,"outputs":[{"output_type":"stream","text":"RF: CV Accuracy  0.9130466721761706\nRF: CV AUC Score  0.9752173370877465\nRF: CV F1 Score  0.9145526427215346\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Train the model\nrf.fit(feats, labels)","execution_count":44,"outputs":[{"output_type":"stream","text":"CPU times: user 22.9 s, sys: 21.9 ms, total: 22.9 s\nWall time: 22.9 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### General Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df['rf_predicted_depression'] = rf.predict(gtwt_feats)","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df.rf_predicted_depression.value_counts() * 100 / len(bef_covid_twt_df)","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"0    82.314394\n1    17.685606\nName: rf_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Covid Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df['rf_predicted_depression'] = rf.predict(covid_twt_feats)","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df.rf_predicted_depression.value_counts() * 100 / len(covid_twt_df)","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"0    82.986425\n1    17.013575\nName: rf_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Depression Classifier Model using Reddit Data - n-grams / tf-idf + LGBM"},{"metadata":{},"cell_type":"markdown","source":"#### Reddit Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(boosting_type='gbdt', n_estimators = 5000, learning_rate=0.03, max_depth=-1,\n                     n_jobs=-1,objective='binary', random_state=42)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint(\"LGBM: CV Accuracy \", cross_val_score(lgbm, feats, labels, scoring='accuracy', cv=5).mean())\n\nprint(\"LGBM: CV AUC Score \", cross_val_score(lgbm, feats, labels, scoring='roc_auc', cv=5).mean())\n\nprint(\"LGBM: CV F1 Score \", cross_val_score(lgbm, feats, labels, scoring='f1', cv=5).mean())","execution_count":51,"outputs":[{"output_type":"stream","text":"LGBM: CV Accuracy  0.930948385965791\nLGBM: CV AUC Score  0.9802327574901977\nLGBM: CV F1 Score  0.931007008505885\nCPU times: user 38min 7s, sys: 15.3 s, total: 38min 22s\nWall time: 9min 47s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Train the model\nlgbm.fit(feats, labels)","execution_count":52,"outputs":[{"output_type":"stream","text":"CPU times: user 2min 58s, sys: 1.12 s, total: 2min 59s\nWall time: 45.6 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.03, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=5000, n_jobs=-1, num_leaves=31, objective='binary',\n               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### General Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df['lgbm_predicted_depression'] = lgbm.predict(gtwt_feats)","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df.lgbm_predicted_depression.value_counts() * 100 / len(bef_covid_twt_df)","execution_count":54,"outputs":[{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"0    79.837121\n1    20.162879\nName: lgbm_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Covid Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df['lgbm_predicted_depression'] = lgbm.predict(covid_twt_feats)","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df.lgbm_predicted_depression.value_counts() * 100 / len(covid_twt_df)","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"0    82.077334\n1    17.922666\nName: lgbm_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Majority Voting"},{"metadata":{},"cell_type":"markdown","source":"#### General Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sum up the classifier voting\nbef_covid_twt_df['sum'] = \\\nbef_covid_twt_df['nb_predicted_depression'] + bef_covid_twt_df['lr_predicted_depression'] + bef_covid_twt_df['rf_predicted_depression'] + bef_covid_twt_df['lgbm_predicted_depression']","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If three or more classifiers voted for depression, then mark it as depression(1). If only one or non classifier voted for depression, then mark it non-depression(0)\n# For ties, follow the best performing model which is the LGBM model\n\nbef_covid_twt_df['majority_voting_predicted_depression'] = np.where(bef_covid_twt_df['sum'] >= 3, 1, 0)\nbef_covid_twt_df['majority_voting_predicted_depression']  = np.where(bef_covid_twt_df['sum'] == 2, bef_covid_twt_df['lgbm_predicted_depression'], \n                                                                 bef_covid_twt_df['majority_voting_predicted_depression'])","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df.majority_voting_predicted_depression.value_counts() * 100 / len(bef_covid_twt_df)","execution_count":71,"outputs":[{"output_type":"execute_result","execution_count":71,"data":{"text/plain":"0    80.625\n1    19.375\nName: majority_voting_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Covid Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sum up the classifier voting\ncovid_twt_df['sum'] = \\\ncovid_twt_df['nb_predicted_depression'] + covid_twt_df['lr_predicted_depression'] + covid_twt_df['rf_predicted_depression'] + covid_twt_df['lgbm_predicted_depression']","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If three or more classifiers voted for depression, then mark it as depression(1). If only one or non classifier voted for depression, then mark it non-depression(0)\n# For ties, follow the best performing model which is the LGBM model\n\ncovid_twt_df['majority_voting_predicted_depression'] = np.where(covid_twt_df['sum'] >= 3, 1, 0)\ncovid_twt_df['majority_voting_predicted_depression']  = np.where(covid_twt_df['sum'] == 2, covid_twt_df['lgbm_predicted_depression'], \n                                                                 covid_twt_df['majority_voting_predicted_depression'])","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df.majority_voting_predicted_depression.value_counts() * 100 / len(covid_twt_df)","execution_count":68,"outputs":[{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"0    84.430276\n1    15.569724\nName: majority_voting_predicted_depression, dtype: float64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Save Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"bef_covid_twt_df.to_csv(\"tweets_in_2009_with_dep_labels.csv\")","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_twt_df.to_csv(\"tweets_in_2020_covid_with_dep_labels.csv\")","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}