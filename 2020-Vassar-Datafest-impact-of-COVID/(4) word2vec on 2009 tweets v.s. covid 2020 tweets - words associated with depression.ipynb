{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import Libraries and Read in Wrangled Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport spacy\nimport nltk\nimport string\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nimport re\nfrom bs4 import BeautifulSoup\nimport unicodedata\nfrom wordcloud import STOPWORDS\nimport gc\nimport re\nimport string\nimport operator\nfrom collections import defaultdict\nfrom time import time  # To time our operations\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n\nimport multiprocessing\nfrom gensim.models import Word2Vec\n\n# from sklearn.pipeline import Pipeline\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler\n\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n# from sklearn.ensemble import RandomForestClassifier\n\n# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Covid Tweets\ncovid_twt_df = pd.read_csv(\"../input/2020-vassar-datafest-josh-data/covid_tweets.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General Tweets from 2009\nbef_covid_twt_df = pd.read_csv(\"../input/2020-vassar-datafest-josh-data/sent_tweet.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # US Gov. response data to COVID\n# us_resp_to_covid_df = pd.read_csv(\"../input/2020-vassar-datafest-josh-data/us_covid_resp.csv\")","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Reddit Depression and non-depression posts for training\n# reddit_dep_df = pd.read_csv(\"../input/reddit-depression-data/preprocessed_data.txt\")","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing for General Tweets & Covid Tweets (post March)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(document):\n    \n    \"\"\"\n    The clean_text function preprocesses the texts in a document/comment\n    \n    Parameters\n    ----------\n    document: the raw text\n    \n    Returns\n    ----------\n    tokens: a list of preprocessed tokens\n    \n    \"\"\"\n    \n    document = ' '.join([word.lower() for word in word_tokenize(document)]) # lowercase texts\n    tokens = word_tokenize(document) # tokenize the document\n    \n    for i in range(0,len(tokens)):\n        # remove whitespaces\n        tokens[i] = tokens[i].strip()\n        # remove html links\n        tokens[i] = re.sub(r'\\S*http\\S*', '', tokens[i]) # remove links with http\n        tokens[i] = re.sub(r'\\S*\\.org\\S*', '', tokens[i]) # remove links with .org\n        tokens[i] = re.sub(r'\\S*\\.com\\S*', '', tokens[i]) # remove links with .com\n        \n        # remove subreddit titles (e.g /r/food)\n        tokens[i] = re.sub(r'S*\\/r\\/\\S*', '' ,tokens[i]) \n        \n        # remove non-alphabet characters\n        tokens[i] = re.sub(\"[^a-zA-Z]+\", \"\", tokens[i])\n        \n        tokens[i] = tokens[i].strip() # remove whitespaces \n        \n        # remove all blanks from the list\n    while(\"\" in tokens): \n        tokens.remove(\"\") \n     \n    return tokens","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cleaning text for General Tweets pre-covid\n\n# call clean_text on df for each row in df\nfor i in range(0,len(bef_covid_twt_df)):\n    # use clean_text on the document/text stored in the content column\n    clean = clean_text(bef_covid_twt_df.loc[i,\"text\"])\n    # joining the tokens together by whitespaces\n    bef_covid_twt_df.loc[i,\"clean_content\"] = ' '.join([token for token in clean])\n    \nbef_covid_twt_df = bef_covid_twt_df.dropna() # remove null data due to some deleted comments\nbef_covid_twt_df = bef_covid_twt_df[bef_covid_twt_df[\"clean_content\"] != ''] # remove blank comments","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cleaning text for Covid Tweets post March\n\n# call clean_text on df for each row in df\nfor i in range(0,len(covid_twt_df)):\n    # use clean_text on the document/text stored in the content column\n    clean2 = clean_text(covid_twt_df.loc[i,\"text\"])\n    # joining the tokens together by whitespaces\n    covid_twt_df.loc[i,\"clean_content\"] = ' '.join([token for token in clean2])\n    \ncovid_twt_df = covid_twt_df.dropna() # remove null data due to some deleted comments\ncovid_twt_df = covid_twt_df[covid_twt_df[\"clean_content\"] != ''] # remove blank comments","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word2vec - build vocabs + train"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.phrases import Phrases, Phraser\n\nsent1 = [row.split() for row in bef_covid_twt_df['clean_content']]\nphrases1 = Phrases(sent1, min_count=30, progress_per=10000)\n\nbigram1 = Phraser(phrases1)\nsentences1 = bigram1[sent1]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent2 = [row.split() for row in covid_twt_df['clean_content']]\nphrases2 = Phrases(sent2, min_count=30, progress_per=10000)\n\nbigram2 = Phraser(phrases2)\nsentences2 = bigram1[sent2]","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting up parameters for word2vec models\n\ncores = multiprocessing.cpu_count( ) # Count the number of cores in a computer\n\nw2v_model1 = Word2Vec(min_count=20,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)\n\nw2v_model2 = Word2Vec(min_count=20,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build word2vec vocabs for pre-covid tweets\n\nt = time()\n\nw2v_model1.build_vocab(sentences1, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":16,"outputs":[{"output_type":"stream","text":"Time to build vocab: 0.04 mins\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build word2vec vocabs for covid tweets\n\nt = time()\n\nw2v_model2.build_vocab(sentences2, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":17,"outputs":[{"output_type":"stream","text":"Time to build vocab: 0.08 mins\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word2vec training for pre-covid tweets\n\nt = time( )\n\nw2v_model1.train(sentences1, total_examples=w2v_model1.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model for pre-covid tweets: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":18,"outputs":[{"output_type":"stream","text":"Time to train the model for pre-covid tweets: 1.04 mins\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word2vec training for covid tweets\n\nt = time( )\n\nw2v_model2.train(sentences2, total_examples=w2v_model2.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model for covid tweets: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":19,"outputs":[{"output_type":"stream","text":"Time to train the model for covid tweets: 1.8 mins\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words associated with word \"depressed\" in pre-covid tweets\n\nw2v_model1.wv.most_similar(positive=[\"depressed\"],topn=50)","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"[('feelin', 0.9882292151451111),\n ('shit', 0.9845561981201172),\n ('a_bit', 0.9830614328384399),\n ('ass', 0.9805903434753418),\n ('being', 0.9800398945808411),\n ('scared', 0.9786115288734436),\n ('too_much', 0.978432834148407),\n ('trying', 0.9778023958206177),\n ('stupid', 0.9772000312805176),\n ('aint', 0.9769729971885681),\n ('hate', 0.9766879081726074),\n ('slow', 0.9765041470527649),\n ('coz', 0.9761098623275757),\n ('feel_like', 0.9755951762199402),\n ('pissed', 0.9753478765487671),\n ('barely', 0.9749050736427307),\n ('sooooo', 0.9748597741127014),\n ('freaking', 0.9748213291168213),\n ('ive', 0.9735394716262817),\n ('badly', 0.9730440974235535),\n ('pain', 0.972820520401001),\n ('bitch', 0.9726149439811707),\n ('happening', 0.9724913835525513),\n ('havent', 0.9717068672180176),\n ('felt', 0.9717012047767639),\n ('dying', 0.9709842801094055),\n ('ughh', 0.9706401228904724),\n ('starving', 0.9705080389976501),\n ('jus', 0.9703129529953003),\n ('thinking', 0.9700263142585754),\n ('lonely', 0.9692559242248535),\n ('thunder', 0.9690951704978943),\n ('completely', 0.9683144688606262),\n ('brain', 0.9682444930076599),\n ('nervous', 0.9664753079414368),\n ('damn', 0.9648555517196655),\n ('craving', 0.9645729064941406),\n ('already', 0.963729977607727),\n ('shut', 0.963622510433197),\n ('getting', 0.9634038209915161),\n ('fast', 0.9628702998161316),\n ('mmm', 0.9624714851379395),\n ('mood', 0.9621472358703613),\n ('bloody', 0.9620701670646667),\n ('fuck', 0.961597740650177),\n ('soo', 0.9615113139152527),\n ('crying', 0.9611783623695374),\n ('missing', 0.961060643196106),\n ('tonite', 0.9609582424163818),\n ('quite', 0.960914134979248)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# words associated with word \"depression\" in pre-covid tweets\n\nw2v_model2.wv.most_similar(positive=[\"depression\"], topn=50)","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"[('anxiety', 0.7966477870941162),\n ('grief', 0.7390024065971375),\n ('stress', 0.6957149505615234),\n ('hiv', 0.6594191193580627),\n ('medtwitter', 0.6236761808395386),\n ('dealing', 0.5988633632659912),\n ('coping', 0.5953640341758728),\n ('fantastic', 0.5952185988426208),\n ('empathy', 0.5915783643722534),\n ('inspired', 0.5849552750587463),\n ('abuse', 0.5780466198921204),\n ('knowledge', 0.5768439769744873),\n ('ebola', 0.5749789476394653),\n ('kills', 0.5742815732955933),\n ('terrible', 0.5715780854225159),\n ('studies', 0.5651646256446838),\n ('emotional', 0.5645834803581238),\n ('image', 0.5640422701835632),\n ('survival', 0.5627989768981934),\n ('teamwork', 0.5625821352005005),\n ('survivor', 0.5575640797615051),\n ('understanding', 0.5552408695220947),\n ('version', 0.5548352003097534),\n ('child', 0.5542569160461426),\n ('position', 0.5504550337791443),\n ('present', 0.546249270439148),\n ('loss', 0.5460538864135742),\n ('damage', 0.5422340035438538),\n ('selfcare', 0.540926992893219),\n ('compassion', 0.5405938029289246),\n ('gain', 0.5392902493476868),\n ('perspective', 0.538345217704773),\n ('mentalhealth', 0.5372310280799866),\n ('sundaythoughts', 0.53066486120224),\n ('energy', 0.5306200385093689),\n ('cancer', 0.5296189188957214),\n ('wisdom', 0.5290782451629639),\n ('climate', 0.5272549390792847),\n ('voices', 0.5272199511528015),\n ('challenging', 0.5249383449554443),\n ('current', 0.5228793025016785),\n ('battling', 0.5149186849594116),\n ('healing', 0.5121400952339172),\n ('contagious', 0.5105148553848267),\n ('brilliant', 0.5099522471427917),\n ('activity', 0.5054100751876831),\n ('foundation', 0.49928081035614014),\n ('uncertainty', 0.498310923576355),\n ('spanish', 0.49807634949684143),\n ('matters', 0.49540281295776367)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}