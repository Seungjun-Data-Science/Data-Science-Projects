{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import Libraries and Read in Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport spacy\nimport nltk\nimport string\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup\n# from contractions import CONTRACTION_MAP\nimport unicodedata\nfrom wordcloud import STOPWORDS\nimport gc\nimport re\nimport string\nimport operator\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nnlp = spacy.load('en', parse = False, tag=False, entity=False)\ntokenizer = ToktokTokenizer()\nstopword_list = nltk.corpus.stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read in fully wrangled data\n\ndf = pd.read_csv(\"../input/cs366-full-data/wrangled_full_dataset.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keeping only \"title\" because we have too much missing \"text\" data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title'] = df['title'].replace({'missing':np.nan})\ndf['text'] = df['text'].replace({'missing':np.nan})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['title'].fillna('missing', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['title'].replace({'missing':np.nan}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stripping HTML tags\n\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing accented characters\n\ndef remove_accent_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Special Characters\n\ndef remove_spec_chars(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatize text\n\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Stopwords\n\ndef remove_stopwords(text, is_lower_case=False):\n    \n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True):\n    \n    normalized_corpus = []\n    # normalize each document in the corpus\n    \n    for doc in corpus:\n        # strip HTML\n        if html_stripping:\n            doc = strip_html_tags(doc)\n            \n        # remove accented characters\n        if accented_char_removal:\n            doc = remove_accent_chars(doc)\n\n        # lowercase the text    \n        if text_lower_case:\n            doc = doc.lower()\n            \n        # remove extra newlines\n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n        \n        # insert spaces between special characters to isolate them    \n        special_char_pattern = re.compile(r'([{.(-)!}])')\n        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n        \n        # lemmatize text\n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n            \n        # remove special characters    \n        if special_char_removal:\n            doc = remove_spec_chars(doc)  \n            \n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        \n        # remove stopwords\n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n            \n        normalized_corpus.append(doc)\n        \n    return normalized_corpus[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new_text = []\n\n# for doc in df['text']:\n#     new_text.append(normalize_corpus([doc], text_lemmatization=False, stopword_removal=False, text_lower_case=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_title = []\n\nfor doc in df['title']:\n    new_title.append(normalize_corpus([doc], text_lemmatization=False, stopword_removal=False, text_lower_case=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['text'] = pd.Series(new_text)\ndf['title'] = pd.Series(new_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic Meta Feature Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['url_len'] = np.where(df['url']=='missing', 0, df['url'].apply(lambda x: len(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word_count\ndf['title_wc'] = df['title'].apply(lambda x: len(str(x).split()))\ndf['title_wc'] = np.where(df['title']=='missing', 0, df['title_wc'])\n\n# unique_word_count\ndf['title_unique_wc'] = df['title'].apply(lambda x: len(set(str(x).split())))\ndf['title_unique_wc'] = np.where(df['title']=='missing', 0, df['title_unique_wc'])\n\n# stop_word_count\ndf['title_stop_wc'] = df['title'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ndf['title_stop_wc'] = np.where(df['title']=='missing', 0, df['title_stop_wc'])\n\n# mean_word_length\ndf['title_mean_wl'] = df['title'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf['title_mean_wl'] = np.where(df['title']=='missing', 0, df['title_mean_wl'])\n\n# median_word_length\ndf['title_median_wl'] = df['title'].apply(lambda x: np.median([len(w) for w in str(x).split()]))\ndf['title_median_wl'] = np.where(df['title']=='missing', 0, df['title_median_wl'])\n\n# char_count\ndf['title_cc'] = df['title'].apply(lambda x: len(str(x)))\ndf['title_cc'] = np.where(df['title']=='missing', 0, df['title_cc'])\n\n# # punctuation_count\n# df['title_pc'] = df['title'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n# df['title_pc'] = np.where(df['title']=='missing', 0, df['title_pc'])\n\n# # hashtag_count\n# df['title_hc'] = df['title'].apply(lambda x: len([c for c in str(x) if c == '#']))\n# df['title_hc'] = np.where(df['title']=='missing', 0, df['title_hc'])\n\n# # mention_count\n# df['title_mc'] = df['title'].apply(lambda x: len([c for c in str(x) if c == '@']))\n# df['title_mc'] = np.where(df['title']=='missing', 0, df['title_mc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"data_meta_feat_added_drop_all_missing.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Class Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing data into real, fake and sarcasm data for easier n-gram visualizations\n\ndf_real = df.copy()[df.target=='real']\ndf_fake = df.copy()[df.target=='fake']\ndf_sarc = df.copy()[df.target=='sarcasm']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bars = sns.countplot(data=df, x='target')\n\nfor bar in bars.patches:\n    plt.gca().text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, str(round(int(bar.get_height())*100/len(df),1)) + ' %', ha='center', color='black', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Class distribution is very skewed towards real news, so we might need to do some resampling to resolve class imbalance issue"},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Number of characters in Title "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(9,5))\nsns.kdeplot(df_real['title_cc'], ax=ax) \nsns.kdeplot(df_fake['title_cc'], ax=ax)\nsns.kdeplot(df_sarc['title_cc'], ax=ax)\nplt.legend(['Real News', 'Fake News', 'Sarcastic News'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Unique Word Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(9,5))\nsns.kdeplot(df_real['title_unique_wc'], ax=ax) \nsns.kdeplot(df_fake['title_unique_wc'], ax=ax)\nsns.kdeplot(df_sarc['title_unique_wc'], ax=ax)\nplt.legend(['Real News', 'Fake News', 'Sarcastic News'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Word Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(9,5))\nsns.kdeplot(df_real['title_wc'], ax=ax) \nsns.kdeplot(df_fake['title_wc'], ax=ax)\nsns.kdeplot(df_sarc['title_wc'], ax=ax)\nplt.legend(['Real News', 'Fake News', 'Sarcastic News'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Average Word Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(9,5))\nsns.kdeplot(df_real['title_mean_wl'], ax=ax) \nsns.kdeplot(df_fake['title_mean_wl'], ax=ax)\nsns.kdeplot(df_sarc['title_mean_wl'], ax=ax)\nplt.legend(['Real News', 'Fake News', 'Sarcastic News'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Median Word Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1,figsize=(9,5))\nsns.kdeplot(df_real['title_median_wl'], ax=ax) \nsns.kdeplot(df_fake['title_median_wl'], ax=ax)\nsns.kdeplot(df_sarc['title_median_wl'], ax=ax)\nplt.legend(['Real News', 'Fake News', 'Sarcastic News'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top N unigram, bigram and trigram viz by type of text (e.g. real, fake and sarcasm)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating ngrams\n\ndef ngram_gen(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unigrams\nreal_title_unigrams = defaultdict(int)\nfake_title_unigrams = defaultdict(int)\nsarc_title_unigrams = defaultdict(int)\n\nfor tweet in df_real['title']:\n    for word in ngram_gen(tweet):\n        real_title_unigrams[word] += 1\n\nfor tweet in df_fake['title']:\n    for word in ngram_gen(tweet):\n        fake_title_unigrams[word] += 1\n\nfor tweet in df_sarc['title']:\n    for word in ngram_gen(tweet):\n        sarc_title_unigrams[word] += 1\n\ndf_real_title_unigrams = pd.DataFrame(sorted(real_title_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_fake_title_unigrams = pd.DataFrame(sorted(fake_title_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_sarc_title_unigrams = pd.DataFrame(sorted(sarc_title_unigrams.items(), key=lambda x: x[1])[::-1])\n\n# df_real_title_unigrams[1] = df_real_title_unigrams[1] * 100 / len(df_real)\n# df_fake_title_unigrams[1] = df_fake_title_unigrams[1] * 100 / len(df_fake)\n# df_sarc_title_unigrams[1] = df_sarc_title_unigrams[1] * 100 / len(df_sarc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bigrams\nreal_title_bigrams = defaultdict(int)\nfake_title_bigrams = defaultdict(int)\nsarc_title_bigrams = defaultdict(int)\n\nfor tweet in df_real['title']:\n    for word in ngram_gen(tweet, n_gram=2):\n        real_title_bigrams[word] += 1\n\nfor tweet in df_fake['title']:\n    for word in ngram_gen(tweet, n_gram=2):\n        fake_title_bigrams[word] += 1\n\nfor tweet in df_sarc['title']:\n    for word in ngram_gen(tweet, n_gram=2):\n        sarc_title_bigrams[word] += 1\n\ndf_real_title_bigrams = pd.DataFrame(sorted(real_title_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_fake_title_bigrams = pd.DataFrame(sorted(fake_title_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_sarc_title_bigrams = pd.DataFrame(sorted(sarc_title_bigrams.items(), key=lambda x: x[1])[::-1])\n\n# df_real_title_bigrams[1] = df_real_title_bigrams[1] * 100 / len(df_real)\n# df_fake_title_bigrams[1] = df_fake_title_bigrams[1] * 100 / len(df_fake)\n# df_sarc_title_bigrams[1] = df_sarc_title_bigrams[1] * 100 / len(df_sarc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trigrams\nreal_title_trigrams = defaultdict(int)\nfake_title_trigrams = defaultdict(int)\nsarc_title_trigrams = defaultdict(int)\n\nfor tweet in df_real['title']:\n    for word in ngram_gen(tweet, n_gram=3):\n        real_title_trigrams[word] += 1\n\nfor tweet in df_fake['title']:\n    for word in ngram_gen(tweet, n_gram=3):\n        fake_title_trigrams[word] += 1\n\nfor tweet in df_sarc['title']:\n    for word in ngram_gen(tweet, n_gram=3):\n        sarc_title_trigrams[word] += 1\n        \ndf_real_title_trigrams = pd.DataFrame(sorted(real_title_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_fake_title_trigrams = pd.DataFrame(sorted(fake_title_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_sarc_title_trigrams = pd.DataFrame(sorted(sarc_title_trigrams.items(), key=lambda x: x[1])[::-1])\n\n# df_real_title_trigrams[1] = df_real_title_trigrams[1] * 100 / len(df_real)\n# df_fake_title_trigrams[1] = df_fake_title_trigrams[1] * 100 / len(df_fake)\n# df_sarc_title_trigrams[1] = df_sarc_title_trigrams[1] * 100 / len(df_sarc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top \"20\"\n\nN = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unigram Visualization\n\nfig, axes = plt.subplots(ncols=3, figsize=(5, 8), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_real_title_unigrams[0].values[:N], x=df_real_title_unigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_fake_title_unigrams[0].values[:N], x=df_fake_title_unigrams[1].values[:N], ax=axes[1], color='blue')\nsns.barplot(y=df_sarc_title_unigrams[0].values[:N], x=df_sarc_title_unigrams[1].values[:N], ax=axes[2], color='pink')\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('Count')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=7)\n    axes[i].tick_params(axis='y', labelsize=7)\n\nplt.title(\"Top 20 most frequent unigrams in Real(left), Fake(middle), and Sarcastic(right) News\")\n\n# axes[0].set_title(f'Top {N} most common unigrams in Real News', fontsize=8)\n# axes[1].set_title(f'Top {N} most common unigrams in Fake News', fontsize=8)\n# axes[2].set_title(f'Top {N} most common unigrams in Sarcastic News', fontsize=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bigram Visualization\n\nfig, axes = plt.subplots(ncols=3, figsize=(8, 9), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_real_title_bigrams[0].values[:N], x=df_real_title_bigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_fake_title_bigrams[0].values[:N], x=df_fake_title_bigrams[1].values[:N], ax=axes[1], color='blue')\nsns.barplot(y=df_sarc_title_bigrams[0].values[:N], x=df_sarc_title_bigrams[1].values[:N], ax=axes[2], color='pink')\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('Count')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=7)\n    axes[i].tick_params(axis='y', labelsize=7)\n\nplt.title(\"Top 20 most frequent bigrams in Real(left), Fake(middle), and Sarcastic(right) News\")\n# axes[0].set_title(f'Top {N} most common bigrams in Real News', fontsize=8)\n# axes[1].set_title(f'Top {N} most common bigrams in Fake News', fontsize=8)\n# axes[2].set_title(f'Top {N} most common bigrams in Sarcastic News', fontsize=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trigram Visualization\n\nfig, axes = plt.subplots(ncols=3, figsize=(12, 9), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_real_title_trigrams[0].values[:N], x=df_real_title_trigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_fake_title_trigrams[0].values[:N], x=df_fake_title_trigrams[1].values[:N], ax=axes[1], color='blue')\nsns.barplot(y=df_sarc_title_trigrams[0].values[:N], x=df_sarc_title_trigrams[1].values[:N], ax=axes[2], color='pink')\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('Count')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=7)\n    axes[i].tick_params(axis='y', labelsize=7)\n\nplt.title(\"Top 20 most frequent trigrams in Real(left), Fake(middle), and Sarcastic(right) News\")\n# axes[0].set_title(f'Top {N} most common trigrams in Real News', fontsize=8)\n# axes[1].set_title(f'Top {N} most common trigrams in Fake News', fontsize=8)\n# axes[2].set_title(f'Top {N} most common trigrams in Sarcastic News', fontsize=8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Real New Data WordCloud based on TFIDF\n\n# %%time\n\n# import sys\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\nfrom wordcloud import WordCloud\nfrom collections import defaultdict\n\ndef get_common_surface_form(original_corpus, stemmer):\n    counts = defaultdict(lambda : defaultdict(int))\n    surface_forms = {}\n\n    for document in original_corpus:\n        for token in document:\n            stemmed = stemmer.stem(token)\n            counts[stemmed][token] += 1\n\n    for stemmed, originals in counts.items():\n        surface_forms[stemmed] = max(originals, \n                                     key=lambda i: originals[i])\n\n    return surface_forms\n\nstemmer = PorterStemmer() # Stemmer for reducing terms to root form \n\nstemmed_corpus = []       # For storing the stemmed tokens \n\noriginal_corpus = []      # For storing the non-stemmed tokens\n\n\nfor contents in df_real['title']:                # Iterate over title column\n\n    tokens = word_tokenize(contents)     # Extract tokens\n\n    stemmed = [stemmer.stem(token) for token in tokens] # Stem tokens\n\n\n    stemmed_corpus.append(stemmed)    # Store stemmed document\n\n    original_corpus.append(tokens)    # Store original document\n    \ndictionary = Dictionary(stemmed_corpus) # Build the dictionary\n\n# Get the surface form for each stemmed word\n\ncounts = get_common_surface_form(original_corpus, stemmer)\n\n# Convert to vector corpus\n\nvectors = [dictionary.doc2bow(text) for text in stemmed_corpus]\n\n# Train TF-IDF model\n\ntfidf = TfidfModel(vectors)\n\n# Get TF-IDF weights\n\nweights = tfidf[vectors[0]]\n\n# Replace term IDs with human consumable strings\n\nweights = [(counts[dictionary[pair[0]]], pair[1]) for pair in weights]\n\n# Initialize the cloud\n\nwc = WordCloud(\n    background_color=\"white\",\n    max_words=2000,\n    width=1024,\n    height=720,\n    stopwords=stopwords.words('english')\n)\n\n# Generate the cloud\nplt.imshow(wc.fit_words(dict(weights)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = PorterStemmer() # Stemmer for reducing terms to root form \n\nstemmed_corpus = []       # For storing the stemmed tokens \n\noriginal_corpus = []      # For storing the non-stemmed tokens\n\n\nfor contents in df_fake['title']:                # Iterate over title column\n\n    tokens = word_tokenize(contents)     # Extract tokens\n\n    stemmed = [stemmer.stem(token) for token in tokens] # Stem tokens\n\n\n    stemmed_corpus.append(stemmed)    # Store stemmed document\n\n    original_corpus.append(tokens)    # Store original document\n    \ndictionary = Dictionary(stemmed_corpus) # Build the dictionary\n\n# Get the surface form for each stemmed word\n\ncounts = get_common_surface_form(original_corpus, stemmer)\n\n# Convert to vector corpus\n\nvectors = [dictionary.doc2bow(text) for text in stemmed_corpus]\n\n# Train TF-IDF model\n\ntfidf = TfidfModel(vectors)\n\n# Get TF-IDF weights\n\nweights = tfidf[vectors[0]]\n\n# Replace term IDs with human consumable strings\n\nweights = [(counts[dictionary[pair[0]]], pair[1]) for pair in weights]\n\n# Initialize the cloud\n\n#from PIL import Image\n# mask = np.array(Image.open(\"../input/fake-news-photo/fake news.jpg\"))\n\nwc = WordCloud(\n    background_color=\"white\",\n    max_words=2000,\n    width=1024,\n    height=720,\n    stopwords=stopwords.words('english')\n)\n\n# Generate the cloud\nplt.imshow(wc.fit_words(dict(weights)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Sarcastic News\n\nstemmer = PorterStemmer() # Stemmer for reducing terms to root form \n\nstemmed_corpus = []       # For storing the stemmed tokens \n\noriginal_corpus = []      # For storing the non-stemmed tokens\n\n\nfor contents in df_sarc['title']:                # Iterate over title column\n\n    tokens = word_tokenize(contents)     # Extract tokens\n\n    stemmed = [stemmer.stem(token) for token in tokens] # Stem tokens\n\n\n    stemmed_corpus.append(stemmed)    # Store stemmed document\n\n    original_corpus.append(tokens)    # Store original document\n    \ndictionary = Dictionary(stemmed_corpus) # Build the dictionary\n\n# Get the surface form for each stemmed word\n\ncounts = get_common_surface_form(original_corpus, stemmer)\n\n# Convert to vector corpus\n\nvectors = [dictionary.doc2bow(text) for text in stemmed_corpus]\n\n# Train TF-IDF model\n\ntfidf = TfidfModel(vectors)\n\n# Get TF-IDF weights\n\nweights = tfidf[vectors[0]]\n\n# Replace term IDs with human consumable strings\n\nweights = [(counts[dictionary[pair[0]]], pair[1]) for pair in weights]\n\n# Initialize the cloud\n\n#from PIL import Image\n# mask = np.array(Image.open(\"../input/fake-news-photo/fake news.jpg\"))\n\nwc = WordCloud(\n    background_color=\"white\",\n    max_words=2000,\n    width=1024,\n    height=720,\n    stopwords=stopwords.words('english')\n)\n\n# Generate the cloud\nplt.imshow(wc.fit_words(dict(weights)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}