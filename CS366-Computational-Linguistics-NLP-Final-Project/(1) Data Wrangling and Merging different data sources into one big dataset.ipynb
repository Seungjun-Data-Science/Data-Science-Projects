{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import Libraries and Read in Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sarcasm Data\nsarcasm_df = \\\npd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\", lines=True)","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PolitiFact Data\npf_fake_df = pd.read_csv(\"../input/fake-news-data/fnn_politics_fake.csv\")\npf_real_df = pd.read_csv(\"../input/fake-news-data/fnn_politics_real.csv\")","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# GossipCop Data\ngc_fake_df = pd.read_csv(\"../input/fakenewsnet-gossipcop/gossipcop_fake.csv\")\ngc_real_df = pd.read_csv(\"../input/fakenewsnet-gossipcop/gossipcop_real.csv\")","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# News Articles from 15 different sites\n\nnews_articles_df = pd.read_csv(\"../input/fake-news-data/news_articles_full.csv\", encoding='unicode_escape')","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Buzzfeed fake/real news data\n\nbuzzf_fake_df = pd.read_csv(\"../input/fakenewsnet/BuzzFeed_fake_news_content.csv\")\nbuzzf_real_df = pd.read_csv(\"../input/fakenewsnet/BuzzFeed_real_news_content.csv\")","execution_count":76,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging different data sources into one big data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create target variables for PolitiFact data\npf_real_df['target'] = 'real'\npf_fake_df['target'] = 'fake'\n\n# Merge fake and real PolitiFact data into one\npf_df = pd.concat([pf_real_df, pf_fake_df])","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Unnecessary Columns from PolitiFact data\npf_df = pf_df.copy()[['news_url','title','target']]\n\n# Change name of columns for easier concatenation later\npf_df.columns = ['url','title','target']","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create target variables for gossipcop data\ngc_real_df['target'] = 'real'\ngc_fake_df['target'] = 'fake'\n\n# Merge fake and real gossipcop data into one\ngc_df = pd.concat([gc_real_df, gc_fake_df])","execution_count":79,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Unnecessary Columns from gossipcop data\ngc_df = gc_df.copy()[['news_url','title','target']]\n\n# Change name of columns for easier concatenation later\ngc_df.columns = ['url','title','target']","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create target variables for Buzzfeed news data\nbuzzf_fake_df['target'] = 'fake'\nbuzzf_real_df['target'] = 'real'\n\n# Merge fake and real buzzfeed news data into one\nbuzzf_df = pd.concat([buzzf_real_df, buzzf_fake_df])","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Unnecessary Columns from Buzzfeed news data\nbuzzf_df = buzzf_df.copy()[['title','text','url','target']]","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change name of columns for easier concatenation later\nsarcasm_df.columns = ['url','title','target']\n\n# Replace 0 and 1s to appropriate target labels (e.g. sarcasm, real)\nsarcasm_df['target'] = sarcasm_df['target'].replace({0:'real',1:'sarcasm'})","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create target variables for news articles data\nnews_articles_df['target'] = 'real'\n\n# Drop Unnecessary Columns from new articles data\nnews_articles_df = news_articles_df.copy()[['article_source_link','title','text', 'target']] \n\n# Change name of columns for easier concatenation later\nnews_articles_df.columns = ['url','title','text','target']","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge all wrangled data sources into one big dataset\n\ndf = pd.concat([news_articles_df, buzzf_df, sarcasm_df, gc_df, pf_df], sort=True)","execution_count":90,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Values"},{"metadata":{},"cell_type":"markdown","source":"Some datasets did not have \"text\" columns. There are some missing \"titles/headlines\" too. For now, we fill them with the character \"Missing\" and we can encode them as a separate category later."},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_feats = ['text','title','url']\nfor feat in missing_feats:\n    df[feat].fillna('missing',inplace=True)","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No more missing values!\ndf.info()","execution_count":93,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 53911 entries, 0 to 431\nData columns (total 4 columns):\ntarget    53911 non-null object\ntext      53911 non-null object\ntitle     53911 non-null object\nurl       53911 non-null object\ndtypes: object(4)\nmemory usage: 2.1+ MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save wrangled data locally for later use\n\ndf.to_csv('wrangled_full_dataset.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}